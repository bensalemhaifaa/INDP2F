<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Infinispan Native CLI</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FpAk3ztC59g/infinispan-cli-image" /><author><name>Ryan Emerson</name></author><id>https://infinispan.org/blog/2021/05/21/infinispan-cli-image</id><updated>2021-05-21T12:00:00Z</updated><content type="html">Starting with Infinispan 12, we provide a natively compiled version of the Infinispan CLI. This can be consumed as a native binary or via a container image. USAGE 1. Start a Infinispan server instance so that the CLI has an endpoint to connect to: docker run -it -p 11222:11222 -e USER="user" -e PASS="pass" quay.io/infinispan/server:12.1 CONTAINER 2. Launch the CLI image and connect to the server image: docker run --net=host -it --rm quay.io/infinispan/cli:12.1 -c http://127.0.0.1:11222 3. Enter the "user" and "pass" credentials, for the username and password respectively. Tip The container’s endpoint is the CLI binary, which lets you pass CLI arguments straight to the image, e.g. quay.io/infinispan/cli:12.1 --version BINARY Native CLI binaries for Linux, Mac and Windows can be downloaded . 2. Download and extract the .zip for your desired platform: a. b. c. 3. Start the CLI and connect to the server container: a. Linux &amp;amp; Mac: ./ispn-cli -c b. Windows (Powershell): &amp;amp; ".\ispn-cli.exe" -c&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FpAk3ztC59g" height="1" width="1" alt=""/&gt;</content><dc:creator>Ryan Emerson</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/05/21/infinispan-cli-image</feedburner:origLink></entry><entry><title>Build your own RPM package with a sample Go program</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/pSogtTpauKs/build-your-own-rpm-package-sample-go-program" /><author><name>asaezmor</name></author><id>6fcd34cf-024f-434e-98bb-33d95c1a994c</id><updated>2021-05-21T07:00:00Z</updated><published>2021-05-21T07:00:00Z</published><summary type="html">&lt;p&gt;A deployment usually involves multiple steps that can be tricky. These days, we have a wide variety of tools to help us create reproducible deployments. In this article, I will show you how easy it is to build a basic RPM package.&lt;/p&gt; &lt;p&gt;We have had package managers for a while. &lt;a href="https://rpm.org/"&gt;RPM&lt;/a&gt; and &lt;a href="http://yum.baseurl.org/"&gt;YUM&lt;/a&gt; simplify installing, updating, or removing a piece of software. However, many companies use package managers only to install software from the operating system vendor and don’t use them for deployments. Creating a package can be daunting at first, but usually, it’s a rewarding exercise that can simplify your pipeline. As a test case, I will show you how to package a simple program written in &lt;a href="https://golang.org/"&gt;Go&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Creating the package&lt;/h2&gt; &lt;p&gt;Many sites rely on configuration managers for deployment. For instance, a typical &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; playbook might be:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; tasks:     - name: 'Copy the artifact'       copy:         src: 'my_app'         dest: '/usr/bin/my_app'           - name: 'Copy configuration files'       template:         src: config.json         dest: /etc/my_app/config.json&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Of course, a real-life playbook will include more steps, like checking the previous installation or handling services. But why not use something like this?&lt;/p&gt; &lt;pre&gt; &lt;code&gt;  tasks:     - name: 'Install my_app'       yum:         name: 'my_app'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let’s see our Go application that serves up a webpage. Here's the  &lt;code&gt;main.go&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;package main import (     "encoding/json"     "flag"     "fmt"     "io/ioutil"     "log"     "net/http" ) type config struct {     Text string `json:"string"` } func main() {     var filename = flag.String("config", "config.json", "")     flag.Parse()     data, err := ioutil.ReadFile(*filename)     if err != nil {         log.Fatalln(err)     }     var config config     err = json.Unmarshal(data, &amp;config)     if err != nil {         log.Fatalln(err)     }     http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {         fmt.Fprintf(w, config.Text)     })     log.Fatal(http.ListenAndServe(":8081", nil)) }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And our &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{     "string": "Hello world :)" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If we run this program, we should see a web page with the &lt;code&gt;config.json&lt;/code&gt; text in port 8081. It's far from production-ready, but it will serve as an example.&lt;/p&gt; &lt;h2&gt;Adding services&lt;/h2&gt; &lt;p&gt;What about a service? Adding services is an excellent way to unify the management of an application, so let’s create our &lt;code&gt;my_app.service&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[Unit] Description=My App [Service] Type=simple ExecStart=/usr/bin/my_app -config /etc/my_app/config.json [Install] WantedBy=multi-user.target&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Every time we want to deploy our application, we need to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Compile the project.&lt;/li&gt; &lt;li&gt;Copy it to &lt;code&gt;/usr/bin/my_app&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Copy the &lt;code&gt;config.json&lt;/code&gt; file to &lt;code&gt;/etc/my_app/config.json&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Copy &lt;code&gt;my_app.service&lt;/code&gt; to &lt;code&gt;/etc/systemd/system/&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Start the service.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Creating the spec file&lt;/h2&gt; &lt;p&gt;Like Ansible, an RPM package needs a definition file where we specify the installation steps, the dependencies, and other things that we might need to install our application on a server:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sudo dnf install git $ sudo dnf module install go-toolset $ sudo dnf groupinstall "RPM Development Tools"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With all of this installed, we are ready to create the package definition file, also known as the &lt;em&gt;spec file&lt;/em&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ rpmdev-newspec my_app.spec&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A spec file can be tricky, but we will keep this simple to appreciate the power of the tool:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Name:           my_app Version:        1.0 Release:        1%{?dist} Summary:        A simple web app License:        GPLv3 Source0:        %{name}-%{version}.tar.gz BuildRequires:  golang BuildRequires:  systemd-rpm-macros Provides:       %{name} = %{version} %description A simple web app %global debug_package %{nil} %prep %autosetup %build go build -v -o %{name} %install install -Dpm 0755 %{name} %{buildroot}%{_bindir}/%{name} install -Dpm 0755 config.json %{buildroot}%{_sysconfdir}/%{name}/config.json install -Dpm 644 %{name}.service %{buildroot}%{_unitdir}/%{name}.service %check # go test should be here... :) %post %systemd_post %{name}.service %preun %systemd_preun %{name}.service %files %dir %{_sysconfdir}/%{name} %{_bindir}/%{name} %{_unitdir}/%{name}.service %config(noreplace) %{_sysconfdir}/%{name}/config.json %changelog * Wed May 19 2021 John Doe - 1.0-1 - First release%changelog &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A few notes:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;Source0&lt;/code&gt; entry can be the source code repository, something like this: &lt;code&gt;https://github.com/&lt;em&gt;user&lt;/em&gt;/my_app/archive/v%&lt;em&gt;version&lt;/em&gt;.tar.gz&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;If you use a URL in &lt;code&gt;Source0&lt;/code&gt;, you can issue &lt;code&gt;spectool -g my_app.spec&lt;/code&gt; to download your source code.&lt;/li&gt; &lt;li&gt;Git allows you to quickly set up a tarball without creating a remote repository: &lt;pre&gt; &lt;code&gt;$ git archive --format=tar.gz --prefix=my_app-1.0/ -o my_app-1.0.tar.gz HEAD&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;The tarball content should look like this: &lt;pre&gt; &lt;code&gt; $tar tf my_app-1.0.tar.gz my_app-1.0/ my_app-1.0/config.json my_app-1.0/main.go my_app-1.0/my_app.service&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Building the RPM&lt;/h2&gt; &lt;p&gt;First, we need to create the &lt;code&gt;rpmbuild&lt;/code&gt; structure and place our tarball inside the source's directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ rpmdev-setuptree $ mv my_app-1.0.tar.gz ~/rpmbuild/SOURCES&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let’s build the RPM for &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; 8:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ rpmbuild -ba my_app.spec&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And that's it!&lt;/p&gt; &lt;p&gt;You should be able to install the RPM now and start the service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sudo dnf install ~/rpmbuild/RPMS/x86_64/my_app-1.0-1.el8.x86_64.rpm $ sudo systemctl start my_app $ curl -L http://localhost:8081&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see the content of our &lt;code&gt;config.json&lt;/code&gt; (which, by the way, is under &lt;code&gt;/etc/my_app&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;But what if we have a new version of our application? We only need to increase the spec file version and build it again. DNF will see that there is a new update available.&lt;/p&gt; &lt;p&gt;And if you are using a package repository, you only need to run &lt;code&gt;dnf update my_app&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you want to delve more into the idea of incorporating RPM files in your deployments, I suggest looking at the &lt;a href="https://rpm-packaging-guide.github.io"&gt;RPM Packaging Guide&lt;/a&gt; and &lt;a href="https://docs.fedoraproject.org/en-US/packaging-guidelines"&gt;the Fedora Packaging Guidelines&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Also, a variety of exciting tools are available to help with the build process or even create repositories for you to use, such as &lt;a href="https://github.com/rpm-software-management/mock"&gt;mock&lt;/a&gt;, &lt;a href="https://pagure.io/fedpkg"&gt;fedpkg&lt;/a&gt;, &lt;a href="https://pagure.io/copr/copr"&gt;COPR&lt;/a&gt;, and &lt;a href="https://pagure.io/koji/"&gt;Koji&lt;/a&gt;. These tools can help you in complex scenarios with multiple dependencies, complex steps, or multiple architectures.&lt;/p&gt; &lt;p&gt;Note that the workflow demonstrated in this article is also applicable to Fedora and to CentOS Stream.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/21/build-your-own-rpm-package-sample-go-program" title="Build your own RPM package with a sample Go program"&gt;Build your own RPM package with a sample Go program&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/pSogtTpauKs" height="1" width="1" alt=""/&gt;</summary><dc:creator>asaezmor</dc:creator><dc:date>2021-05-21T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/21/build-your-own-rpm-package-sample-go-program</feedburner:origLink></entry><entry><title type="html">What's new in Vert.x 4.1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/iKD6ltptEsg/whats-new-in-vert-x-4-1" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/whats-new-in-vert-x-4-1</id><updated>2021-05-21T00:00:00Z</updated><content type="html">See an overview of all new and exciting features in Vert.x 4.1, including futures, distributed tracing, reactive database clients, SQL templating, and more.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/iKD6ltptEsg" height="1" width="1" alt=""/&gt;</content><dc:creator>Julien Viet</dc:creator><feedburner:origLink>https://vertx.io/blog/whats-new-in-vert-x-4-1</feedburner:origLink></entry><entry><title>Shenandoah garbage collection in OpenJDK 16: Concurrent reference processing</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/kLbotXu8hOI/shenandoah-garbage-collection-openjdk-16-concurrent-reference-processing" /><author><name>Roman Kennke</name></author><id>d9c065a8-086a-41f0-97b0-c321b82bf650</id><updated>2021-05-20T07:00:00Z</updated><published>2021-05-20T07:00:00Z</published><summary type="html">&lt;p&gt;The primary motivation behind the &lt;a href="https://openjdk.java.net/projects/shenandoah/"&gt;Shenandoah garbage collection (GC) project&lt;/a&gt; in the OpenJDK was to reduce garbage collection pause times. Reference processing has traditionally been one of the primary contributors to GC pauses. The relationship is mostly linear: The more references the application is churning, the higher is the impact on garbage collection pauses and latency. The key here is "churning," or how many references need to be processed at every GC cycle. The references with referents that never die, or that die along with references themselves, are not a problem.&lt;/p&gt; &lt;p&gt;I have myself recommended in the past that if you care about latency, you had better not churn soft, weak, and phantom references or finalizees. In this article, I want to show why reference processing has contributed to &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; garbage collection pauses in the past, and how we solved that problem by making reference processing concurrent in JDK 16.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: If your application churns through soft, weak, or phantom references or finalizees, JDK 16 with its concurrent reference processing in Shenandoah GC might significantly improve your application's latency.&lt;/p&gt; &lt;h2&gt;References recap&lt;/h2&gt; &lt;p&gt;The term &lt;em&gt;references&lt;/em&gt; in this article means Java objects of type &lt;code&gt;java.lang.ref.Reference&lt;/code&gt; and its subtypes &lt;code&gt;SoftReference&lt;/code&gt;, &lt;code&gt;WeakReference&lt;/code&gt;, &lt;code&gt;PhantomReference&lt;/code&gt;, and &lt;code&gt;FinalReference&lt;/code&gt; (more on the last one later). Regular references between objects are also called &lt;em&gt;strong references&lt;/em&gt;. Each reference points to one referent. The purpose of the various reference types is to be able to reference an object, but not keep that object from being reclaimed by the reference. Reclamation follows reachability rules, which are specified roughly as follows (in order of decreasing reachability):&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;SoftReference&lt;/code&gt;: The referent can be reclaimed as soon as it is no longer reachable by any strong reference. Reclamation is additionally subject to other heuristics; e.g., usually soft references are not expected to be reclaimed unless memory pressure is high. This makes soft references suitable for memory-sensitive (cache) implementations. A rather bad choice, but a choice nonetheless: GC would clear soft references when memory is tight.&lt;/li&gt; &lt;li&gt;&lt;code&gt;WeakReference&lt;/code&gt;: The referent can be reclaimed as soon as it is no longer reachable by any strong or soft reference. As long as it has not been reclaimed, the referent may still be accessed, at which point it becomes strongly reachable again.&lt;/li&gt; &lt;li&gt;&lt;code&gt;FinalReference&lt;/code&gt;: You probably don't know this one, because it is a JDK-internal reference type. It is used to implement &lt;code&gt;Object.finalize()&lt;/code&gt;. Any object that implements &lt;code&gt;finalize()&lt;/code&gt; becomes the referent of a &lt;code&gt;FinalReference&lt;/code&gt; and is registered in a corresponding &lt;code&gt;ReferenceQueue&lt;/code&gt;. As soon as that object is no longer reachable by strong, soft, or weak references, it is processed and its &lt;code&gt;finalize()&lt;/code&gt; method is called.&lt;/li&gt; &lt;li&gt;&lt;code&gt;PhantomReference&lt;/code&gt;: The referent may be reclaimed as soon as it is no longer reachable by any strong, soft, weak, or final reference; in other words, when it is properly unreachable. The referent can never be accessed. This is done so that the referent cannot be accidentally resurrected after it has been determined to be unreachable. As soon as the reachability of referents has been determined, unreachable referents get cleared (set to null) and their corresponding reference objects get enqueued in a respective &lt;code&gt;ReferenceQueue&lt;/code&gt; for further processing by the application. Phantom references are typically used for managing resources such as native memory or operating system file handles that could otherwise not be handled by GC.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Traditional reference processing&lt;/h2&gt; &lt;p&gt;In versions prior to JDK 16, Shenandoah discovers and processes references in a way that closely follows the reachability rules in the previous section:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;First we determine the reachability of all strongly reachable objects during concurrent marking. As soon as the marking wavefront reaches a reference object that does not still have a marked referent (i.e., an object that is not strongly reachable from somewhere else), it stops there, and enqueues the reference in one of four &lt;em&gt;discovery queues&lt;/em&gt; for later processing. There is one discovery queue per reference type. A special case concerns soft references: the GC may decide, based on heuristics (e.g., memory pressure and/or object age), whether to treat soft references like strong references.&lt;/li&gt; &lt;li&gt;Next, as soon as concurrent (strong) marking is complete, the JVM stops the application and starts processing the discovery queues: &lt;ol&gt;&lt;li&gt;All &lt;code&gt;SoftReference&lt;/code&gt; objects are inspected. If the referent is not strongly reachable (i.e., marked by concurrent marking), it is cleared, and the &lt;code&gt;SoftReference&lt;/code&gt; is put in the processing queue.&lt;/li&gt; &lt;li&gt;All &lt;code&gt;WeakReference&lt;/code&gt; objects are inspected. If the referent is not strongly reachable, it is cleared, and the &lt;code&gt;WeakReference&lt;/code&gt; is put in the processing queue.&lt;/li&gt; &lt;li&gt;All &lt;code&gt;FinalReference&lt;/code&gt; objects are inspected. If the referent is not strongly reachable, the &lt;code&gt;FinalReference&lt;/code&gt; is put in the processing queue, but the referent is not yet cleared, because it is still needed later so that the GC can call its &lt;code&gt;finalize()&lt;/code&gt; method. Also, something special happens now: starting from the otherwise unreachable referent, marking is resumed, and the subgraph of objects found from the referent is marked. This is important in the next step, to avoid reclaiming &lt;code&gt;PhantomReference&lt;/code&gt; objects that are reachable from finalizees. This additional marking pass is problematic because it happens while the JVM is stopped, and it is theoretically bounded only by the live data set size. In other words, we may spend a lot of time here marking the subgraph from finalizees.&lt;/li&gt; &lt;li&gt;All &lt;code&gt;PhantomReference&lt;/code&gt; objects are inspected. If the referent is not reachable (neither strongly nor from finalizees), the referent is cleared, and the &lt;code&gt;PhantomReference&lt;/code&gt; is put in the processing queue.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Finally, the processing queue is added to a Java linked list for further &lt;code&gt;ReferenceQueue&lt;/code&gt; processing.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;From these steps, it follows that the contribution of reference processing to GC pause time is basically proportional to &lt;em&gt;number-of-processed-references + size-of-newly-marked-subgraph&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;In order to address the pause-time problem of reference processing, we need the GC to do it concurrently. The task of reference processing can be divided into two subtasks: the concurrent marking of referents, including their respective subgraphs, and the concurrent processing and clearing of references. Those two subtasks are entangled in the traditional implementation, so let's see how we can untangle them.&lt;/p&gt; &lt;h3&gt;Concurrent reference marking&lt;/h3&gt; &lt;p&gt;Upon closer inspection of the traditional implementation, we find that we can simplify our reachability model. We don't really have five levels of reachability (strong, soft, weak, final, phantom), but only two. Let's look at the criteria for classifying references from a different angle:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;SoftReference&lt;/code&gt; objects get cleared and enqueued when the referent is not strongly reachable and we meet a certain heuristic. In essence, we can decide, before or during concurrent marking, whether the &lt;code&gt;SoftReference&lt;/code&gt; should be treated like a strong reference or a weak reference.&lt;/li&gt; &lt;li&gt;&lt;code&gt;WeakReference&lt;/code&gt; objects get cleared and enqueued when the referent is not strongly reachable.&lt;/li&gt; &lt;li&gt;&lt;code&gt;FinalReference&lt;/code&gt; objects get enqueued when the referent is not strongly reachable.&lt;/li&gt; &lt;li&gt;&lt;code&gt;PhantomReference&lt;/code&gt;s get cleared and enqueued when the referent is not strongly reachable and not reachable from any &lt;code&gt;FinalReference&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In other words, our two relevant reachability levels are &lt;em&gt;strongly reachable&lt;/em&gt; and &lt;em&gt;finalizably reachable&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The traditional implementation determines reachability by marking in the following steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;During concurrent marking, we establish the set of all strongly reachable objects.&lt;/li&gt; &lt;li&gt;We process all references—soft, weak, and final—that require only this reachability level.&lt;/li&gt; &lt;li&gt;We continue marking from finalizees.&lt;/li&gt; &lt;li&gt;We process the remaining phantom references that also require this new information.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Can we determine both strong and finalizable reachability concurrently? Sure we can. But we need to extend our marking bitmap a little bit: Instead of using one bit per object (marked versus unmarked), we now need two bits to represent all possible states: strongly reachable, finalizably reachable, and unreachable (and a 4th state that is used internally). Equipped with this information, we can now concurrently mark through all the live objects and determine both strong and finalizable reachability:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Start marking from roots with normal strong wavefront.&lt;/li&gt; &lt;li&gt;As soon as a strong wavefront encounters a &lt;code&gt;SoftReference&lt;/code&gt;, decide (based on heuristics) whether it should be treated as a strong or weak reference. If the soft references should be treated as strong, normally mark through the referent (marking its subgraph as strong), otherwise stop the wavefront there and enqueue the soft reference in the discovered queue.&lt;/li&gt; &lt;li&gt;When a strong wavefront encounters a &lt;code&gt;WeakReference&lt;/code&gt; or &lt;code&gt;PhantomReference&lt;/code&gt;, stop the wavefront there and enqueue the reference in the discovered queue.&lt;/li&gt; &lt;li&gt;As soon as a strong wavefront encounters a &lt;code&gt;FinalReference&lt;/code&gt;, mark that &lt;code&gt;FinalReference&lt;/code&gt; as strong, and switch to the finalizable wavefront for marking the referent. All objects reachable from there will now be marked finalizable.&lt;/li&gt; &lt;li&gt;When a strong wavefront encounters an object that is already marked finalizable, upgrade the object and its subgraph to strong.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;We now have concurrently marked all reachable objects and determined whether they are strongly or finalizably reachable. We also have enqueued all reference objects in our discovered queue for further processing. This solves the first half of the problem. We still need to clear the unreachable referents and enqueue the reference objects.&lt;/p&gt; &lt;h3&gt;Concurrent reference processing&lt;/h3&gt; &lt;p&gt;We established reachability concurrently during marking. This is finished in the final-mark pause, and we also set up for evacuation during that pause. Concurrent reference processing happens at the beginning of the concurrent evacuation phase. We need to do two things:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Scan the discovered queue and clear all unreachable referents.&lt;/li&gt; &lt;li&gt;Enqueue "unreachable" reference objects in a processing queue for further processing on the Java side.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The tasks are mostly as simple as they sound: We concurrently scan the discovered queue, inspect each reference object, and see whether the referent is reachable. We follow the reachability rules cited early: to be reachable, soft, weak, and final references must be strongly reachable, while phantom references must be either strongly or finalizably reachable. If a reference is unreachable, clear the referent and put the reference into the processing queue.&lt;/p&gt; &lt;p&gt;But wait: What if the Java program tries to access the referent before we cleared it? It would still see the referent that we determined to be otherwise unreachable, and would thus resurrect it. This would be a violation of the spec and cause all sorts of troubles, up to JVM crashes, because the GC would subsequently reclaim or override that object, and the Java program would end up with a dangling pointer.&lt;/p&gt; &lt;p&gt;The solution to this problem is a special barrier that we insert in &lt;code&gt;Reference.get()&lt;/code&gt;. We already have an LRB there because it is a reference load:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;// Pseudocode of Reference.get() intrinsic T Reference_get() { T ref = this.referent; return lrb(ref); } Let's return null when the referent is unreachable: // Pseudocode of Reference.get() for concurrent reference processing T Reference_get() { T ref = this.referent; if (isUnreachable(ref) { return null; } return lrb(ref); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Voila!&lt;/em&gt; We now always return &lt;code&gt;null&lt;/code&gt; whenever we try to access an unreachable referent, and the Java application never gets to resurrect an object by accident.&lt;/p&gt; &lt;h2&gt;Enough theory, show me the numbers&lt;/h2&gt; &lt;p&gt;How bad is concurrent garbage collection, actually?&lt;/p&gt; &lt;p&gt;Let's look at a workload that makes modest use of references and finalizees, running under JDK 11. The &lt;code&gt;-Xlog:gc+stats&lt;/code&gt; option gives us some statistics:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;Pause Final Mark (N) = 0,278 s (a = 3812 us) (n = 73) (lvls, us = 893, 2891, 3535, 4414, 7707) Finish Queues = 0,020 s (a = 273 us) (n = 73) (lvls, us = 94, 123, 137, 227, 4426) Weak References = 0,214 s (a = 2929 us) (n = 73) (lvls, us = 158, 2109, 2695, 3555, 6072) Process = 0,213 s (a = 2924 us) (n = 73) (lvls, us = 154, 2109, 2695, 3555, 6067)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This shows that out of the average final-mark pause of 3.8ms, GC spends 2.9ms in weak reference processing, and in the worst case even 4.4ms out of 7.7ms.&lt;/p&gt; &lt;p&gt;Let's look at the same code with JDK 16:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;Pause Final Mark (G) = 0,208 s (a = 2044 us) (n = 102) (lvls, us = 688, 820, 914, 1562, 36975) Pause Final Mark (N) = 0,089 s (a = 870 us) (n = 102) (lvls, us = 500, 625, 705, 764, 4489) Finish Queues = 0,034 s (a = 329 us) (n = 102) (lvls, us = 107, 125, 174, 246, 4072) Update Region States = 0,004 s (a = 38 us) (n = 102) (lvls, us = 26, 34, 38, 41, 52) Manage GC/TLABs = 0,001 s (a = 13 us) (n = 102) (lvls, us = 9, 13, 13, 14, 22) Choose Collection Set = 0,019 s (a = 183 us) (n = 102) (lvls, us = 97, 156, 188, 203, 323) Rebuild Free Set = 0,002 s (a = 23 us) (n = 102) (lvls, us = 13, 20, 23, 25, 29) Initial Evacuation = 0,028 s (a = 274 us) (n = 102) (lvls, us = 133, 207, 227, 260, 3264) E: = 0,247 s (a = 2422 us) (n = 102) (lvls, us = 799, 1875, 2031, 2090, 39583) E: Thread Roots = 0,247 s (a = 2422 us) (n = 102) (lvls, us = 799, 1875, 2031, 2090, 39583) Concurrent Weak References = 0,967 s (a = 9479 us) (n = 102) (lvls, us = 152, 4824, 9473, 11914, 39231) Process = 0,966 s (a = 9470 us) (n = 102) (lvls, us = 148, 4805, 9453, 11914, 39215) [&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Reference processing is completely gone from the final-mark pause, and the pause is down to 0.9ms. Reference processing is now listed separately under "Concurrent Weak References" and takes an average of 9.5ms—but we don't care all that much because it doesn't keep the application from running.&lt;/p&gt; &lt;p&gt;These numbers are from a relatively modest workload. We have worked with a customer workload with literally millions of weak references and finalizees, and as is to be expected, the effect is much more dramatic there.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you are using weak references, soft reference caches, finalizees, or phantom references for resource cleanups, and you care about garbage collection pauses and latency, you might want to upgrade to JDK 16 and give Shenandoah GC a try.&lt;/p&gt; &lt;p&gt;For more information about Java, please refer to the related &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Red Hat Developer topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/20/shenandoah-garbage-collection-openjdk-16-concurrent-reference-processing" title="Shenandoah garbage collection in OpenJDK 16: Concurrent reference processing"&gt;Shenandoah garbage collection in OpenJDK 16: Concurrent reference processing&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/kLbotXu8hOI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Roman Kennke</dc:creator><dc:date>2021-05-20T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/20/shenandoah-garbage-collection-openjdk-16-concurrent-reference-processing</feedburner:origLink></entry><entry><title type="html">Rule Impact Analysis</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/D8Zmjp4sa6M/rule-impact-analysis.html" /><author><name>Toshiya Kobayashi</name></author><id>https://blog.kie.org/2021/05/rule-impact-analysis.html</id><updated>2021-05-20T06:45:00Z</updated><content type="html">After you develop rules and put the system into production, you will need to maintain the rules to keep up with business requirements. Basically tests should ensure the correctness and integrity of the updated rules, but while you work on updating the rules, you might want to know the "impact" of your changes. Rule impact analysis feature helps you. drools-impact-analysis is a new experimental module to analyze impacts of changes in rules, which is available since drools 7.54.0.Final. drools-impact-analysis parses rules and visualizes the relationships between the rules. Also it can render impacted rules when you specify a rule which you plan to change. Let’s see how to use this feature. USING THE IMPACT ANALYSIS FEATURE Example code can be found in * Have drools-impact-analysis-graph-graphviz in your project dependency &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-impact-analysis-graph-graphviz&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; * Create a KieFileSystem to store your assets. Then call KieBuilder.buildAll(ImpactAnalysisProject.class). You can get AnalysisModel. // set up KieFileSystem ... KieBuilder kieBuilder = KieServices.Factory.get().newKieBuilder(kfs).buildAll(ImpactAnalysisProject.class); ImpactAnalysisKieModule analysisKieModule = (ImpactAnalysisKieModule) kieBuilder.getKieModule(); AnalysisModel analysisModel = analysisKieModule.getAnalysisModel(); * Convert the AnalysisModel to Graph using ModelToGraphConverter ModelToGraphConverter converter = new ModelToGraphConverter(); Graph graph = converter.toGraph(analysisModel); * Specify a rule which you plan to change. ImpactAnalysisHelper will produce a sub graph which contains the changed rule and impacted rules ImpactAnalysisHelper impactFilter = new ImpactAnalysisHelper(); Graph impactedSubGraph = impactFilter.filterImpactedNodes(graph, "org.drools.impact.analysis.example.PriceCheck_11"); * Generate a graph image using GraphImageGenerator. You can choose the format from DOT, SVG and PNG. GraphImageGenerator generator = new GraphImageGenerator("example-impacted-sub-graph"); generator.generateSvg(impactedSubGraph); * Simple text output is also available using TextReporter. You can choose the format from HierarchyText and FlatText. String hierarchyText = TextReporter.toHierarchyText(impactedSubGraph); System.out.println(hierarchyText); TIPS A typical use case is to view an impacted sub graph because a whole graph could be too large if you have many rules. Red node is a changed rule. Yellow nodes are impacted rules. Solid arrow represents positive impact, where the source rule activates the target rule. Dashed arrow represents negative impact, where the source rule deactivates the target rule. Dotted arrow represents unknown impact, where the source rule may activate or deactivate the target rule. You can collapse the graph based on rule name prefix (= RuleSet in spreadsheet) using GraphCollapsionHelper. It will help you to see the overview. You can also use ImpactAnalysisHelper to the collapsed graph. Graph collapsedGraph = new GraphCollapsionHelper().collapseWithRuleNamePrefix(graph); Graph impactedCollapsedSubGraph = impactFilter.filterImpactedNodes(collapsedGraph, "org.drools.impact.analysis.example.PriceCheck"); You can filter the relations by giving positiveOnly to true for ModelToGraphConverter, ImpactAnalysisHelper and GraphCollapsionHelper constructor. So you can view only positive relations. ModelToGraphConverter converter = new ModelToGraphConverter(true); Graph graph = converter.toGraph(analysisModel); ImpactAnalysisHelper impactFilter = new ImpactAnalysisHelper(true); Graph impactedSubGraph = impactFilter.filterImpactedNodes(graph, "org.drools.impact.analysis.example.PriceCheck_11"); If the number of rules is very large, text output would be useful. [*] is a changed rule. [+] is impacted rules. A rule with parentheses means a circular reference so it doesn’t render further. --- toHierarchyText --- Inventory shortage[+] PriceCheck_11[*] StatusCheck_12[+] (Inventory shortage) StatusCheck_13[+] StatusCheck_11[+] (PriceCheck_11) --- toFlatText --- Inventory shortage[+] PriceCheck_11[*] StatusCheck_11[+] StatusCheck_12[+] StatusCheck_13[+] Note: drools-impact-analysis can parse DRLs and related assets (e.g. spreadsheets). DMN is not subject to this feature but you know that DMN already renders relationships between decisions using DRD. Rule impact analysis is a brand new experimental feature so there is much room to improve (e.g. usability). We really appreciate your feedback. Thanks! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/D8Zmjp4sa6M" height="1" width="1" alt=""/&gt;</content><dc:creator>Toshiya Kobayashi</dc:creator><feedburner:origLink>https://blog.kie.org/2021/05/rule-impact-analysis.html</feedburner:origLink></entry><entry><title>Authorizing multi-language microservices with oauth2-proxy</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hrU1e-JIgnY/authorizing-multi-language-microservices-oauth2-proxy" /><author><name>Rarm Nagalingam</name></author><id>8205e5c8-5bf0-42ab-a3e6-f650632c3fd5</id><updated>2021-05-20T03:00:00Z</updated><published>2021-05-20T03:00:00Z</published><summary type="html">&lt;p&gt;In an article published in August 2020, &lt;a href="https://developers.redhat.com/blog/2020/08/03/authorizing-multi-language-microservices-with-louketo-proxy/"&gt;Authorizing multi-language microservices with Louketo Proxy&lt;/a&gt;, I explained how to use Louketo Proxy to provide authentication and authorization to your &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;. Since then, the Louketo Proxy project has reached its end of life, with developers recommending the &lt;a href="https://oauth2-proxy.github.io/oauth2-proxy/"&gt;oauth2-proxy&lt;/a&gt; project as an alternative.&lt;/p&gt; &lt;p&gt;In this article, I will outline how to secure a microservice with &lt;a href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt; and oauth2-proxy.&lt;/p&gt; &lt;h2&gt;Using Keycloak&lt;/h2&gt; &lt;p&gt;The following sections describe how to set up Keycloak on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; for the scenario in this article.&lt;/p&gt; &lt;h3&gt;Test and deploy the Keycloak server&lt;/h3&gt; &lt;p&gt;Begin by testing the Keycloak server. Use the following commands to deploy the Keycloak server on OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export PROJECT="keyauth" $ oc new-project ${PROJECT} $ oc new-app --name sso \ --docker-image=quay.io/keycloak/keycloak \ -e KEYCLOAK_USER='admin' \ -e KEYCLOAK_PASSWORD='oauth2-demo' \ -e PROXY_ADDRESS_FORWARDING='true' \ -n ${PROJECT} $ oc create route edge --service=sso -n ${PROJECT}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We now need to add a client configuration to Keycloak so it can secure our application.&lt;/p&gt; &lt;h3&gt;Create and configure the oauth2 authentication client&lt;/h3&gt; &lt;p&gt;Log in to Keycloak with the username &lt;code&gt;admin&lt;/code&gt; and password &lt;code&gt;oauth2-demo&lt;/code&gt;. On the Keycloak user interface (UI), select &lt;strong&gt;Clients&lt;/strong&gt; on the left navigation bar and select &lt;strong&gt;Create&lt;/strong&gt;. On the &lt;strong&gt;Add Client&lt;/strong&gt; page, fill out all the necessary fields, and click &lt;strong&gt;Save&lt;/strong&gt; to create a new client, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/01_create_client.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/01_create_client.png?itok=_GRhUbWk" width="600" height="292" title="01_create_client" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 1: Add the required field values on the Add Client page on the Keycloak UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After adding the new client, go to the &lt;strong&gt;Oauth2-proxy&lt;/strong&gt; page, and switch the client's &lt;strong&gt;Access Type&lt;/strong&gt; field from public to confidential, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/02_confidential.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/02_confidential.png?itok=wKcNFscm" width="600" height="437" title="02_confidential" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 2: Switch the Access Type field (client protocol) from public to confidential on the Oauth2-proxy page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, set a valid callback URL for our oauth2-proxy securing our application. In this scenario, oauth2-proxy is securing a flask app. The URL is similar to the Keycloak URL, although instead of the prefix &lt;code&gt;sso&lt;/code&gt;, it uses &lt;code&gt;flask&lt;/code&gt;. For example, if your Keycloak URL is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;https://sso-keyauth.apps-crc.testing&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;you should set the following URL in the &lt;strong&gt;Valid Redirect URIs&lt;/strong&gt; field:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;https://flask-keyauth.apps-crc.testing/oauth2/callback&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 3 shows the dialog to create the redirect URL.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/03_callback_url.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/03_callback_url.png?itok=FR8iztq5" width="560" height="205" title="03_callback_url" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 3: Set a valid redirect callback URL using flask and take note of the generated secret.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Then click &lt;strong&gt;Save&lt;/strong&gt; on the client protocol of confidential, the page displays a new tab, &lt;strong&gt;Credentials&lt;/strong&gt;. Click the tab and take note of the generated secret&lt;/p&gt; &lt;h3&gt;Configure the mappers&lt;/h3&gt; &lt;p&gt;Keycloak supports passing group memberships of users to the microservice as &lt;strong&gt;X-Forwarded-Groups&lt;/strong&gt;. This can be enabled by configuring a group mapper. This is a useful way to expose authorization functions within the microservice. For example we could restrict privileged functions of the microservices to users in the admin group.&lt;/p&gt; &lt;p&gt;Select the &lt;strong&gt;Mappers&lt;/strong&gt; tab on the &lt;strong&gt;Create Protocol Mapper&lt;/strong&gt; page, add a new mapper and enter all the groups using the following settings:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: &lt;code&gt;groups&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mapper Type&lt;/strong&gt;: Group Membership&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token Claim Name&lt;/strong&gt;: &lt;code&gt;groups&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full group path&lt;/strong&gt;: OFF&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Click &lt;strong&gt;Save&lt;/strong&gt; when the fields are complete. Figure 4 shows the &lt;strong&gt;Create Protocol Mapper&lt;/strong&gt; page with all fields complete.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/04_groups_mapper.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/04_groups_mapper.png?itok=UVl1MG8K" width="413" height="420" title="04_groups_mapper" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 4: Create a new mapper with all the necessary field values to the microservice for X-Forwarded-Groups.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Configure the user groups&lt;/h3&gt; &lt;p&gt;Now select &lt;strong&gt;Groups&lt;/strong&gt; from the left navigation bar and add two groups:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;admin&lt;/li&gt; &lt;li&gt;basic_user&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Configure the user&lt;/h3&gt; &lt;p&gt;From the left navigation bar, select &lt;strong&gt;Users&lt;/strong&gt; and add a user. Enter an email address and a password for the new user, and add the user to the &lt;strong&gt;basic_user&lt;/strong&gt; and &lt;strong&gt;admin&lt;/strong&gt; groups you've just created. Now you are ready to configure oauth2-proxy and the example application.&lt;/p&gt; &lt;h2&gt;Deploy the application with oauth2-proxy sidecar&lt;/h2&gt; &lt;p&gt;Now let's deploy our application with an oauth2-proxy sidecar. To make life easier, here is a Git repository with all of the OpenShift templates. Clone the repository and &lt;code&gt;cd&lt;/code&gt; into the folder and run the following script to configure and deploy all of the templates, simply passing the OpenShift project name as a &lt;code&gt;variable&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/snowjet/demo-oauth2-proxy.git $ cd demo-oauth2-proxy # deploy flask with oauth2-proxy in project keyauth $ ./create_app.sh keyauth&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The script above deploys a number of templates for the application. However, the important templates relating to oauth2-proxy are the configuration maps.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;Configmap_ssopubkey.yml&lt;/code&gt; contains the Keycloak Public Certificate to validate the JSON Web Token (JWT) passed by oauth2-proxy&lt;/li&gt; &lt;li&gt;&lt;code&gt;Configmap-oauth.yml&lt;/code&gt; contains the variables required to configure oauth2-proxy. These include: &lt;ul&gt;&lt;li&gt;&lt;code&gt;oidc_issuer_url&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;client_secret&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;redirect_url&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;cookie_secret&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;whitelist_domains&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;cookie_domains&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The remaining templates complete the deployment:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;DeploymentConfig&lt;/code&gt; for the application with a sidecar for oauth2-proxy&lt;/li&gt; &lt;li&gt;Service pointing to the oauth2-proxy&lt;/li&gt; &lt;li&gt;Route pointing to the service for the oauth2-proxy&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Testing the configuration&lt;/h2&gt; &lt;p&gt;Once deployed, test the configuration and then browse to the example application. Remember to sign out of the admin portion of Keycloak before trying to sign in to the Keycloak web app. Then, on the &lt;strong&gt;Sign in&lt;/strong&gt; screen, click &lt;strong&gt;Sign in with Keycloak&lt;/strong&gt;, as shown in Figure 5.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/05_sign_in.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/05_sign_in.png?itok=vi7u-8cC" width="487" height="148" title="05_sign_in" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 5: Sign in to test the configuration and browse to the example application.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once you are redirected to the Keycloak login dialog, type the username or email address and the password for the application user, as shown in Figure 6. Then click the login button to complete your entries.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/06_login_sso.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/06_login_sso.png?itok=ErbDGHkV" width="600" height="494" title="06_login_sso" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 6: On the Keycloak login dialog, type the username or email address and password for the application user.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After you are successfully authenticated, you will be redirected to an application page that returns a JSON file like the one shown in Figure 7. The file exposes the headers passed along by oauth2-proxy:&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/07_json.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/07_json.png?itok=fY4zh11f" width="600" height="457" title="07_json" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 7: Redirects to an application page that returns a JSON file.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article was an introduction to oauth2-proxy, a replacement for Louketo Proxy that provides authentication for your applications without making you code OpenID Connect clients within microservices. As always, I welcome your questions and any feedback and details you share in the comments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/20/authorizing-multi-language-microservices-oauth2-proxy" title="Authorizing multi-language microservices with oauth2-proxy"&gt;Authorizing multi-language microservices with oauth2-proxy&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hrU1e-JIgnY" height="1" width="1" alt=""/&gt;</summary><dc:creator>Rarm Nagalingam</dc:creator><dc:date>2021-05-20T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/20/authorizing-multi-language-microservices-oauth2-proxy</feedburner:origLink></entry><entry><title type="html">Quarkus 2.0.0.Alpha3 released - GraalVM 21.1, SmallRye GraphQL Client...</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/31arKh3NBTg/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-2-0-0-alpha3-released/</id><updated>2021-05-20T00:00:00Z</updated><content type="html">We just released our third Alpha for Quarkus 2.0 which contains several enhancements and new features: #17014 - Update to GraalVM 21.1 #17350 - SmallRye GraphQL Client extension + 1.2.1 upgrade #16857 - Add Amazon SSM extension #16963 - Add quarkus:test goal #17232 - Introduce coroutine support in Reactive Messaging...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/31arKh3NBTg" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-2-0-0-alpha3-released/</feedburner:origLink></entry><entry><title>Red Hat Enterprise Linux 8.4 now generally available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KURwpOeOG7A/red-hat-enterprise-linux-84-now-generally-available" /><author><name>Don Pinto</name></author><id>4096faef-e827-4fa2-be19-c29377974e32</id><updated>2021-05-19T13:00:00Z</updated><published>2021-05-19T13:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux 8.4&lt;/a&gt;, which was pre-announced on April 27 at &lt;a href="https://www.redhat.com/en/summit"&gt;Red Hat Summit&lt;/a&gt;, is now generally available. We encourage &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; developers to download this latest release and try out the new software. We also recommend updating both development, and production systems to the new &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux &lt;/a&gt;(RHEL) 8.4 release.&lt;/p&gt; &lt;h2&gt;What's new in RHEL 8.4?&lt;/h2&gt; &lt;p&gt;RHEL 8.4 delivers a streamlined path from development to deployment that unifies teams across a single open platform, including the tools and analytics needed to build and manage these systems on any footprint—from the data center to the cloud to &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;the edge&lt;/a&gt;, and beyond. With access to the latest tools, programming languages, and enhanced container capabilities, development teams can achieve faster time to value when producing new code. You can learn more about what RHEL 8.4 provides &lt;a href="https://www.redhat.com/en/blog/rhel-84-brings-continuous-stability-plus-innovation"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you’re a developer, here are some key highlights that you need to know about Red Hat Enterprise Linux 8.4.&lt;/p&gt; &lt;h3&gt;Leverage the latest database technology&lt;/h3&gt; &lt;p&gt;Now, you can leverage the latest database technology in your applications:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;PostgreSQL 13&lt;/strong&gt;, now available through RHEL application streams, improves database performance and lets developers modernize their applications, especially in the cloud.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Redis 6&lt;/strong&gt;, now available in RHEL application streams, allows developers to build modern apps that can leverage the new database security enhancements, and client-side caching features to boost performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MariaDB 10.5&lt;/strong&gt;, now available in RHEL application streams, lets you build apps that can leverage added database features, including IPv6 (INET 6) data type support, more granular privileges, and clustering with the Galera plugin.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Power your applications with the latest application runtimes&lt;/h3&gt; &lt;p&gt;Application runtime updates bring new features to your applications:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Python 3.9&lt;/strong&gt; brings several new enhancements including timezone-aware timestamps, the new string prefix, and suffix methods, and dictionary union operations, so that developers can modernize their apps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Go 1.15&lt;/strong&gt; brings improved memory allocation for small objects, improvements to the Go linker, and several other core library improvements.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rust 1.49&lt;/strong&gt; allows developers to write high-performance applications that run with a low memory footprint making it highly suitable for edge use cases. Additionally, Rust is a statically typed language making it easy to catch errors at compile-time and maintain.&lt;/li&gt; &lt;li&gt;With the latest &lt;strong&gt;LLVM toolset&lt;/strong&gt;, developers can take advantage of fresher tooling, and compatibility with other code built with compatible versions of &lt;a href="https://developers.redhat.com/search?t=clang+llvm"&gt;LLVM/Clang&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Build, share, and collaborate on RHEL applications with your friends and customers&lt;/h3&gt; &lt;p&gt;Container updates make it easier to build, share, and collaborate on RHEL applications:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Red Hat Universal Base Image (&lt;/strong&gt;&lt;a href="https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image"&gt;&lt;strong&gt;UBI&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;): &lt;/strong&gt;The certified language runtime containers have been updated and include containerized environments for some of the languages listed above. Additionally, with RHEL 8.4, a new, micro UBI container offering is available in the catalog to provide an even smaller than minimal base container image.&lt;/li&gt; &lt;li&gt;Looking for a specific container image? Check out the &lt;a href="https://connect.redhat.com/explore/red-hat-container-certification"&gt;&lt;strong&gt;Red Hat Certified Containers&lt;/strong&gt;&lt;/a&gt; through the &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt; Red Hat Ecosystem Catalog&lt;/a&gt;. This makes it easier to build and deploy mission-critical applications using the supported application streams for Red Hat Enterprise Linux and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt; Red Hat OpenShift &lt;/a&gt; environments.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Get started with RHEL 8.4 today&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux 8.4 continues Red Hat’s commitment to customer choice in terms of the underlying compute architecture, with availability across x86_64, ppc64le, s390x, and aarch64 hardware.&lt;/p&gt; &lt;p&gt;Developers with active subscriptions can access &lt;a href="https://access.redhat.com/downloads/content/479/ver=/rhel---8/8.4/x86_64/product-software"&gt;Red Hat Enterprise Linux downloads&lt;/a&gt;. If you’re new to using Red Hat products, register for the Red Hat developer program to get access to the &lt;a href="https://developers.redhat.com/rhel8"&gt;&lt;strong&gt;Individual Developer subscription&lt;/strong&gt;&lt;/a&gt; for RHEL, which can be used in production for up to 16 systems. For information about a &lt;a href="https://www.redhat.com/en/blog/new-year-new-red-hat-enterprise-linux-programs-easier-ways-access-rhel#Bookmark%202"&gt;&lt;strong&gt;Red Hat Developer for Teams subscription&lt;/strong&gt;&lt;/a&gt;, contact your Red Hat account representative.&lt;/p&gt; &lt;p&gt;For more information, please read the full &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/"&gt;release notes.&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/19/red-hat-enterprise-linux-84-now-generally-available" title="Red Hat Enterprise Linux 8.4 now generally available"&gt;Red Hat Enterprise Linux 8.4 now generally available&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KURwpOeOG7A" height="1" width="1" alt=""/&gt;</summary><dc:creator>Don Pinto</dc:creator><dc:date>2021-05-19T13:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/19/red-hat-enterprise-linux-84-now-generally-available</feedburner:origLink></entry><entry><title>micropipenv: Installing Python dependencies in containerized applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JW_9dWQdZIE/micropipenv-installing-python-dependencies-containerized-applications" /><author><name>Fridolin Pokorny, Lumír Balhar</name></author><id>0c2b09c1-86ab-496d-afa1-48aaac77e981</id><updated>2021-05-19T03:00:00Z</updated><published>2021-05-19T03:00:00Z</published><summary type="html">&lt;p&gt;Trends in the software engineering industry show that the &lt;a href="https://insights.stackoverflow.com/survey/2020#technology-programming-scripting-and-markup-languages"&gt;Python programming language is growing in popularity&lt;/a&gt;. Properly managing &lt;a href="https://developers.redhat.com/blog/category/python/"&gt;Python&lt;/a&gt; dependencies is crucial to guaranteeing a healthy software development life cycle. In this article, we will look at installing Python dependencies for Python applications into &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized environments&lt;/a&gt;, which also have become very popular. In particular, we introduce &lt;a href="https://pypi.org/project/micropipenv"&gt;micropipenv&lt;/a&gt;, a tool we created as a compatibility layer on top of pip (the Python package installer) and related installation tools. The approach discussed in this article ensures that your applications are shipped with the desired software for the purposes of traceability or integrity. The approach provides reproducible Python applications across different application builds done over time.&lt;/p&gt; &lt;h2&gt;Python dependency management&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/open-source"&gt;Open source&lt;/a&gt; community efforts provide tools to manage application dependencies. The most popular such tools for Python are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://pypi.org/project/pip/"&gt;pip&lt;/a&gt; (offered by the Python Packaging Authority)&lt;/li&gt; &lt;li&gt;&lt;a href="https://pypi.org/project/pip-tools/"&gt;pip-tools&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pypi.org/project/pipenv/"&gt;Pipenv&lt;/a&gt; (offered by the Python Packaging Authority)&lt;/li&gt; &lt;li&gt;&lt;a href="https://pypi.org/project/poetry/"&gt;Poetry&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Each of these tools has its own pros and cons, so developers can choose the right tooling based on their preferences.&lt;/p&gt; &lt;h3&gt;Virtual environment management&lt;/h3&gt; &lt;p&gt;One of the features that can be important for developers is implicit virtual environment management, offered by Pipenv and Poetry. This feature is a time-saver when developing applications locally, but can have drawbacks when installing and providing the application in a container image. One of the drawbacks of adding this layer is its potentially &lt;a href="https://github.com/fridex/s2i-example-micropipenv#micropipenv-vs-pipenv-size"&gt;negative impact on container-image size&lt;/a&gt; because the tools add bulk to the software within the container image.&lt;/p&gt; &lt;p&gt;On the other hand, pip and pip-tools require explicit virtual environment management when developing applications locally. With explicit virtual environment management, the application dependencies do not interfere with system Python libraries or other dependencies shared across multiple projects.&lt;/p&gt; &lt;h3&gt;The lock file&lt;/h3&gt; &lt;p&gt;Even though pip is the most fundamental tool for installing Python dependencies, it does not provide an implicit mechanism for managing the whole dependency graph. This gave the pip-tools developers an opportunity to design pip-tools to manage a locked-down dependency listing, featuring direct dependencies and transitive dependencies based on application requirements.&lt;/p&gt; &lt;p&gt;Stating all the dependencies in the lock file provides fine-grained control over which Python dependencies in which versions are installed at any point in time. If developers do not lock down all the dependencies, they might confront issues that can arise over time due to new Python package releases, &lt;a href="https://www.python.org/dev/peps/pep-0592/"&gt;yanking specific Python releases (PEP-592)&lt;/a&gt;, or the complete removal of Python packages from Python package indexes such as &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt;. All of these actions can introduce undesired and unpredictable issues created by changes across releases in the dependencies installed. Maintaining and shipping the lock file with the application avoids such issues and provides traceability to application maintainers and developers.&lt;/p&gt; &lt;h3&gt;Digests of installed artifacts&lt;/h3&gt; &lt;p&gt;Even though pip-tools states all the dependencies that are installed in specific versions in its lock file, we recommend including digests of installed artifacts by providing the &lt;a href="https://pypi.org/project/pip-tools/"&gt;--generate-hashes option to the pip-compile command&lt;/a&gt;, because this is not done by default. The option triggers integrity checks of installed artifacts during the installation process. Digests of installed artifacts are automatically included in lock files managed by Pipenv or Poetry.&lt;/p&gt; &lt;p&gt;On the other hand, pip cannot generate hashes of packages that are already installed. However, pip performs checks during the installation process when digests of artifacts are provided explicitly or when you supply the &lt;a href="https://pip.pypa.io/en/stable/cli/pip_install/"&gt;--require-hashes option&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To support all the tools discussed in this section, we introduced micropipenv. The rest of this article  explains how it works and how it fits into the Python environment.&lt;/p&gt; &lt;h2&gt;Installing Python dependencies with micropipenv&lt;/h2&gt; &lt;p&gt;micropipenv parses the requirements or lock files produced by the tools discussed in the previous section: &lt;code&gt;requirements.txt&lt;/code&gt;, &lt;code&gt;Pipfile&lt;/code&gt;/&lt;code&gt;Pipfile.lock&lt;/code&gt;, and &lt;code&gt;pyproject.toml&lt;/code&gt;/&lt;code&gt;poetry.lock&lt;/code&gt;. micropipenv tightly cooperates with the other tools and acts as a small addition to pip that can prepare dependency installation while respecting requirements files and lock files. All the main benefits of the core pip installation process stay untouched.&lt;/p&gt; &lt;p&gt;By supporting all the files produced by pip, pip-tools, Pipenv, and Poetry, micropipenv allows users to employ the tool of their choice for installing and managing Python dependencies in their projects. Once the application is ready to be shipped within the container image, developers can seamlessly use all recent &lt;a href="https://github.com/sclorg/s2i-python-container"&gt;Python Source-To-Image&lt;/a&gt; (S2I) container images based on Python 3. These images offer micropipenv functionality. You can subsequently use the applications in deployments managed by &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 1 shows micropipenv as a common layer for installing Python dependencies in an OpenShift deployment.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/01/Screenshot_2021-01-07_14-56-15.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/01/Screenshot_2021-01-07_14-56-15.png?itok=bu2OPWT-" width="600" height="258" title="micropipenv in OpenShift s2i" typeof="Image" /&gt;&lt;/a&gt; &lt;figcaption class="rhd-media-caption field__item"&gt; micropipenv serving common layer in OpenShift Python S2I. &lt;/figcaption&gt;&lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 1: micropipenv in an OpenShift Python S2I deployment.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To enable micropipenv in the Python &lt;a href="https://docs.openshift.com/container-platform/4.6/builds/build-strategies.html#builds-strategy-s2i-build_build-strategies"&gt;S2I build process&lt;/a&gt;, export the &lt;code&gt;ENABLE_MICROPIPENV=1&lt;/code&gt; environment variable. See the &lt;a href="https://github.com/sclorg/s2i-python-container/tree/master/3.6#environment-variables"&gt;documentation for more details&lt;/a&gt;. This feature is available in all recent &lt;a href="https://github.com/sclorg/s2i-python-container/"&gt;S2I container images&lt;/a&gt; based on Python 3 and built on top of Fedora, CentOS Linux, &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images&lt;/a&gt; (UBI), or &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL). Even though micropipenv was originally designed for containerized Python S2I container images, we believe that &lt;a href="https://github.com/thoth-station/micropipenv/blob/deed020fb3ee57ab1aa7df8a46419393b501d7f3/README.rst#micropipenv-use-cases"&gt;it will find use cases elsewhere&lt;/a&gt;, such as when installing dependencies without lock file management tools or when converting between lock files of different types. We also found the tool &lt;a href="https://github.com/thoth-station/jupyterlab-requirements/"&gt;suitable for assisting with dependency installation in Jupyter Notebook&lt;/a&gt; to &lt;a href="https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/"&gt;support reproducible data science environments&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please check the &lt;a href="https://www.youtube.com/watch?v=I-QC83BcLuo&amp;t=8m58s"&gt;Project Thoth scrum demo&lt;/a&gt; if you are interested in Thoth's Python S2I integration (the micropipenv demo starts at 9:00). You can also check the &lt;a href="https://www.youtube.com/watch?v=QT4ConYPSOA"&gt;Improvements in OpenShift S2I&lt;/a&gt; talk presented at the &lt;a href="https://developers.redhat.com/devnation"&gt;DevNation 2019&lt;/a&gt; conference. &lt;a href="https://github.com/thoth-station/talks/blob/master/2020-09-25-devconf-us/thoth_python_s2i.pdf"&gt;Slides&lt;/a&gt; and a &lt;a href="https://devconfus2020.sched.com/event/dkXi/improvements-in-openshift-python-s2i"&gt;description of the talk&lt;/a&gt; are also available online.&lt;/p&gt; &lt;h2&gt;Benefits of micropipenv&lt;/h2&gt; &lt;p&gt;We wanted to bring Pipenv or Poetry to the Python S2I build process because of their advantages for developers. But from the perspective of RPM package maintenance, both Pipenv and Poetry are hard to package and maintain in the standard way we use RPM in Fedora, CentOS, and RHEL. Pipenv bundles all of its more than 50 dependencies, and the list of these dependencies changes constantly. Package tools like these are complex, so maintaining them and fixing all possible security issues in their bundled dependencies could be very time-consuming.&lt;/p&gt; &lt;p&gt;Moreover, micropipenv brings unified installation logs. Logs are not differentiated based on the tool used, and provide insights into issues that can arise during installation.&lt;/p&gt; &lt;p&gt;Another reason for micropipenv is that a Pipenv installation uses more than 18MB of disk space, which is a lot for a tool we need to use only once during the container build.&lt;/p&gt; &lt;p&gt;We have already packaged and prepared &lt;a href="https://src.fedoraproject.org/rpms/micropipenv"&gt;micropipenv as an RPM package&lt;/a&gt;. It is also &lt;a href="https://pypi.org/project/micropipenv"&gt;available on PyPI&lt;/a&gt;; the project is open source and developed on GitHub in the &lt;a href="https://github.com/thoth-station/micropipenv/"&gt;thoth-station/micropipenv repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;At its core, micropipenv depends only on pip. Other features are available when TOML Python libraries are installed (&lt;a href="https://pypi.org/project/toml"&gt;toml&lt;/a&gt; or legacy &lt;a href="https://pypi.org/project/pytoml"&gt;pytoml&lt;/a&gt; are optional dependencies). The minimal dependencies give micropipenv a very light feel, overall. Having just &lt;a href="https://github.com/thoth-station/micropipenv/blob/master/micropipenv.py"&gt;one file in the codebase&lt;/a&gt; helps with project maintenance, in contrast to the much larger Pipenv or Poetry codebases. All the installation procedures are reused from supported &lt;a href="https://pypi.org/project/pip"&gt;pip releases&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Using and developing micropipenv&lt;/h2&gt; &lt;p&gt;You can get micropipenv in any of the following ways:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Providing &lt;a href="https://github.com/sclorg/s2i-python-container/tree/master/3.6#environment-variables"&gt;ENABLE_MICROPIPENV=1 to the Source-To-Image&lt;/a&gt; container build process&lt;/li&gt; &lt;li&gt;&lt;a href="https://src.fedoraproject.org/rpms/micropipenv"&gt;Installing a micropipenv RPM&lt;/a&gt; by running: &lt;pre&gt; $ dnf install micropipenv&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Installing a &lt;a href="https://pypi.org/project/micropipenv"&gt;micropipenv Python package&lt;/a&gt; by running: &lt;pre&gt; $ pip install micropipenv&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To develop and improve micropipenv or submit feature requests, please visit the &lt;a href="https://github.com/thoth-station/micropipenv/"&gt;thoth-station/micropipenv&lt;/a&gt; repository.&lt;/p&gt; &lt;h2&gt;Acknowledgment&lt;/h2&gt; &lt;p&gt;micropipenv was developed at the &lt;a href="http://github.com/aicoe"&gt;Red Hat Artificial Intelligence Center of Excellence&lt;/a&gt; in &lt;a href="http://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt; and brought to you thanks to cooperation with the Red Hat Python maintenance team.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/19/micropipenv-installing-python-dependencies-containerized-applications" title="micropipenv: Installing Python dependencies in containerized applications"&gt;micropipenv: Installing Python dependencies in containerized applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JW_9dWQdZIE" height="1" width="1" alt=""/&gt;</summary><dc:creator>Fridolin Pokorny, Lumír Balhar</dc:creator><dc:date>2021-05-19T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/19/micropipenv-installing-python-dependencies-containerized-applications</feedburner:origLink></entry><entry><title type="html">Kogito User Task Process API</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/EqXdsfWLnrc/kogito-user-tasks-process-api.html" /><author><name>Francisco Javier Tirado Sarti</name></author><id>https://blog.kie.org/2021/05/kogito-user-tasks-process-api.html</id><updated>2021-05-18T08:54:00Z</updated><content type="html">It can be argued that the greatest technical advances in the history of humankind (agriculture, wheel, steam machine, printing…) have augmented production while reducing human effort. This does not imply that these advances have removed human intervention altogether. On the contrary, they have allowed humans to focus on the most relevant parts of the production process, those places where human genius can make a difference. In agriculture, humans are pivotal in the discovery of new breeds; wheeled vehicles propelled by steam machines derivatives still need human drivers to reach their intended destination and printers will be useless without authors willing to write books. Kogito is not an exception. Although the BPMN ecosystem is all about the pursuit of automation, there is still much needed room for direct human intervention in most business processes. Aware of that circumstance, BPMN contributors defined User Task activity: “A User Task is a typical “workflow” Task where a human performer performs the Task with the assistance of a software application and is scheduled through a task list manager of some sort”. A User Task in Kogito is also known as a Human Task. I will be using both terms indistinguishably. This is the first post of a series where I will be discussing Kogito functionality related with User Task. In this one, I will focus on the process API.  A Human Task might be considered a state machine, with at least one initial and one final state. A Human Task might also be viewed as a function, because given a set of inputs it will produce a set of results.  The life cycle of Human Task typically consists of a sequence of phases, starting with an Active phase, which every created task is initialized to, and finishing with a Complete phase. Between these initial and final states, a Task can go through an arbitrary number of phases, including no phase at all between Active and Complete. The fact that there is an arbitrary number of phases is an important difference from jBPM, where all possible transitions were predefined and immutable, while in Kogito it is up to the final user to decide which transitions are valid, by creating its own custom phases. A User Task can also be interpreted as a human function, where a human, after looking at the input parameters, might add additional information in the form of output parameters or results.  The set of key value pairs that might act as input to the task or be a result of the task completion is known as the Task Model. In Kogito, the Task Model is predefined and generated as part of the compilation of the  process definition. One consequence of what was stated in the previous paragraph is that a human cannot  add more information to the task model without changing process definition. The exception to that rule are comments and attachments:  * A comment consists of a human readable text that will help to achieve a successful resolution of the task.  * An attachment is a reference to an external URIs containing information relevant to the task, for example a screen snapshot.  Note the way attachments are implemented in Kogito is completely  different from the approach taken in jBPM. In jBPM, attachment content was expected to be , while in Kogito, just the URI reference is saved. When during process execution, a User Task node is reached, relevant process properties are passed as input parameters to the task and  the process is paused till the task is completed through human intervention. Once the task reaches its Complete phase, results produced by that task are mapped to process properties and the process execution resumes. KOGITO RUNTIME TASK API Once a task becomes active, it has to be eventually completed to resume the process that instantiates it. Therefore, users should be able to change the current task phase, update tasks results or perform both operations at the same time. They should  also be able to manage comments and attachments. Kogito provides REST APIs to fulfill all these requirements. Let’s split them in functional groups.  PHASE TRANSITIONS The REST template to transition from the current phase to another one is POST http://&lt;host:port&gt;/&lt;process id&gt;/&lt;process instance&gt;/&lt;task name&gt;/&lt;task instance id&gt;/phase/&lt;phase name&gt; As request body, you might optionally provide a JSON object whose key value pairs will be added to the task results.  I guess there are too many template replacements to do, so let’s analyze them one by one, starting with &lt;phase name&gt; Besides initial (Active) and final (Completed) states, Kogito provide these predefined phases ClaimAllow the user to take ownership of the taskReleaseAllow the owner to free ownership of the taskAbortThe task is aborted and process will finish with failureSkipThe task is skipped and process will resume execution In addition to them, Kogito allows users to define their custom phases, as described . There are two pivotal methods to consider when adding a custom phase: mandatory public boolean canTransition(LifeCyclePhase phase), which determines if the transition is allowed from current to target phase and optional void apply(KogitoWorkItem workitem, Transition&lt;?&gt; transition), which gives the user freedom to modify the task information (workitem) using the information passed as body in the REST invocation (transition). You have a nice example of the powerful capabilities (including defining your own security policies) of apply method in implementation.  The rest of template substitutions depend on the process being used, hence I introduce you probably the simplest BPMN using tasks in the world, the one.  Using this process, the value for process id is approval and for task name is firstLineApproval or secondLineApproval.  process instance id should be obtained from the output of the call that starts the process (see for more details on that).  Once the process is started, a task instance for firstLineApproval will be created and the process suspended. To obtain that task instance id, you need to retrieve the list of active tasks: GET http://&lt;host:port&gt;/approvals/&lt;process instance id&gt;/tasks. This API will return a list of Task Model instances. Task Model is a POJO generated from process definition during its compilation. It contains information about the current task phase, tasks input parameters and task output results. Similar to the Task Model, input and output parameters are modelled as generated POJOs, each POJO containing a getter/setter pair per parameter.  In approval process, as you can see in the companion diagrams, both firstLineApproval and secondLineApproval tasks share the same model.  Input data consists of a user defined POJO named . Output is just  a boolean field called approved.  Hence the generated input and output POJOS for firstLineApproval and secondLineApproval are: public class Approvals_1_TaskInput { .... @UserTaskParam(value = ParamType.INPUT) private org.acme.travels.Traveller traveller; public org.acme.travels.Traveller getTraveller() { return traveller; } public void setTraveller(org.acme.travels.Traveller traveller) { this.traveller = traveller; } } public class Approvals_1_TaskOutput implements org.kie.kogito.MapOutput { ..... @UserTaskParam(value = ParamType.OUTPUT) private java.lang.Boolean approved; public java.lang.Boolean getApproved() { return approved; } public void setApproved(java.lang.Boolean approved) { this.approved = approved; } } Therefore, the response of the call performed to retrieve the list of task instances will look this  [ { "id": "07f0f804-030b-4c50-8b61-684db9b748a4", "name": "firstLineApproval", "state": 0, "phase": "active", "phaseStatus": "Ready", "parameters": { "traveller": { "firstName": "John", "lastName": "Doe", "email": "jon.doe@example.com", "nationality": "American", "address": { "street": "main street", "city": "Boston", "zipCode": "10005", "country": "US" } } }, "results": { "approved": null } } ] As expected, the response contains only one active firstlineApproval (field name value). We are particularly interested in the id field, which value is the one to be used as task instance id. Since the task was just created (phaseStatus field is Ready),  there are not any results yet, hence the approved field has a value of null. Besides that, it is worth to mention that the parameters field contains a traveller instance, which was passed as part of the process starting request. Now we have all the information we need to perform the phase transition REST invocation. Assuming you have deployed the process into your host at default port, the call to complete a task for this particular process instance will be (task instance id is highlighted in orange) : URIhttp://localhost:8080/approvals/&lt;process instance id&gt;/firstLineApproval/07f0f804-030b-4c50-8b61-684db9b748a4/phase/completeBody{“approved”:true} Once this call is performed, the task will be completed (not longer being returned by tasks GET  APIs), its output will be set to {“approved”:true} and the process execution will be resumed.   SAVING RESULTS A human task is a potentially long one, so, from time to time it might be wise to save the progress of it, or, as unfortunately happened to me with this post, you might be ending doing it twice.  Saving a human task means adding results to it without performing a phase transition, operation that might be performed by passing as the whole output model as body of this call PUT http://&lt;host:port&gt;/approvals/&lt;process instance id&gt;/&lt;task name&gt;/&lt;task instance id&gt; Note that you need to pass the whole output model (in our approval example that’s not make any difference, since the output model consists of just one parameter) because by convention PUT, as an idempotent method, implies replacing the whole resource.  You can check the model information about an active task at any moment by using GET http://&lt;host:port&gt;/approvals/&lt;process instance id&gt;/&lt;task name&gt;/&lt;task instance id&gt; COMMENTS In Kogito we consider that comments are entities in their own right, so they deserve a dedicated, although not independent, REST resource for them. That way all CRUD operations are supported over them, as illustrated below.  MethodTemplate URIDescriptionGET/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/comments/&lt;comment_id&gt;Retrieve list of comments associated to the taskPUT/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/comments/&lt;comment_id&gt;Updates a comment with the text passed in the bodyPOST/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/commentsCreates a comment with the text passed in the body. Returns the comment idDELETE/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/comments/&lt;comment_id&gt;Deletes the comment ATTACHMENTS Same rationale that was used for comments applies to attachments, the only differences being that the resource name is attachments rather than comments and that the body should be a valid URI, not any random string.  MethodTemplate URIDescriptionGET/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/attachmentsRetrieve list of attachments associated to the taskPUT/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/attachments/&lt;attachment_id&gt;Updates an attachment with the URI passed in the bodyPOST/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/attachmentsCreates an attachment with the URI passed in the body. Returns the attachment idDELETE/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/attachments/&lt;attachment_id&gt;Deletes the attachment CONCLUSION In this post we have gone over the APIs provided by Kogito to operate with tasks from a process perspective. In the next post we will discuss the Task management API, which allows users with enough privileges to change task information not related with the process model, in other words, task fields that are present for any task, regardless process definition. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/EqXdsfWLnrc" height="1" width="1" alt=""/&gt;</content><dc:creator>Francisco Javier Tirado Sarti</dc:creator><feedburner:origLink>https://blog.kie.org/2021/05/kogito-user-tasks-process-api.html</feedburner:origLink></entry></feed>
